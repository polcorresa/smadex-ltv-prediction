{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smadex LTV Prediction - Exploratory Data Analysis\n",
    "\n",
    "**Competition**: Smadex Datathon 2025  \n",
    "**Task**: Predict 7-day in-app purchase revenue (`iap_revenue_d7`)\n",
    "\n",
    "This notebook explores:  \n",
    "1. Dataset structure and statistics\n",
    "2. Target variable distribution\n",
    "3. Feature distributions and correlations\n",
    "4. Temporal patterns\n",
    "5. Whale user characteristics\n",
    "6. Feature engineering opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data (sample for EDA)\n",
    "train_path = Path('../data/raw/train')\n",
    "\n",
    "# Use Dask for large dataset\n",
    "ddf = dd.read_parquet(train_path, engine='pyarrow')\n",
    "\n",
    "# Sample 100k rows for EDA\n",
    "df = ddf.sample(frac=0.01, random_state=42).compute()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column types\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target statistics\n",
    "target = 'iap_revenue_d7'\n",
    "\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Mean: ${df[target].mean():.2f}\")\n",
    "print(f\"Median: ${df[target].median():.2f}\")\n",
    "print(f\"Std: ${df[target].std():.2f}\")\n",
    "print(f\"Min: ${df[target].min():.2f}\")\n",
    "print(f\"Max: ${df[target].max():.2f}\")\n",
    "print(f\"\\n% Zero revenue: {(df[target] == 0).mean() * 100:.2f}%\")\n",
    "print(f\"% Non-zero revenue: {(df[target] > 0).mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Raw distribution\n",
    "axes[0, 0].hist(df[target], bins=100, edgecolor='black')\n",
    "axes[0, 0].set_title('Raw Revenue Distribution')\n",
    "axes[0, 0].set_xlabel('Revenue ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Non-zero only\n",
    "non_zero = df[df[target] > 0][target]\n",
    "axes[0, 1].hist(non_zero, bins=100, edgecolor='black')\n",
    "axes[0, 1].set_title('Non-Zero Revenue Distribution')\n",
    "axes[0, 1].set_xlabel('Revenue ($)')\n",
    "\n",
    "# Log-transformed\n",
    "axes[1, 0].hist(np.log1p(df[target]), bins=100, edgecolor='black')\n",
    "axes[1, 0].set_title('Log-Transformed Revenue')\n",
    "axes[1, 0].set_xlabel('log(1 + revenue)')\n",
    "\n",
    "# Boxplot\n",
    "axes[1, 1].boxplot(non_zero, vert=True)\n",
    "axes[1, 1].set_title('Boxplot (Non-Zero)')\n",
    "axes[1, 1].set_ylabel('Revenue ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile analysis\n",
    "quantiles = [0.5, 0.75, 0.9, 0.95, 0.99, 1.0]\n",
    "quantile_values = df[target].quantile(quantiles)\n",
    "\n",
    "print(\"Revenue Quantiles:\")\n",
    "for q, v in zip(quantiles, quantile_values):\n",
    "    print(f\"  {q*100:.0f}th percentile: ${v:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observations**:\n",
    "- Highly zero-inflated (majority of users have $0 revenue)\n",
    "- Right-skewed distribution (long tail of high spenders)\n",
    "- Log transformation improves normality\n",
    "- Top 5% of users drive significant revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Buyer vs. Non-Buyer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buyer classification\n",
    "df['is_buyer'] = (df[target] > 0).astype(int)\n",
    "\n",
    "buyer_count = df['is_buyer'].sum()\n",
    "buyer_rate = df['is_buyer'].mean()\n",
    "\n",
    "print(f\"Buyers: {buyer_count:,} ({buyer_rate*100:.2f}%)\")\n",
    "print(f\"Non-buyers: {len(df) - buyer_count:,} ({(1-buyer_rate)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buyer vs. Non-buyer comparison\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols = [c for c in numerical_cols if c not in [target, 'is_buyer', 'row_id']]\n",
    "\n",
    "comparison = df.groupby('is_buyer')[numerical_cols[:10]].mean()\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "correlations = df[numerical_cols].corrwith(df[target]).abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 Correlated Features:\")\n",
    "print(correlations.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap (top features)\n",
    "top_features = correlations.head(15).index.tolist() + [target]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    df[top_features].corr(),\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=1\n",
    ")\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Whale User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whale users (top 5% revenue)\n",
    "whale_threshold = df[target].quantile(0.95)\n",
    "df['is_whale'] = (df[target] >= whale_threshold).astype(int)\n",
    "\n",
    "whale_count = df['is_whale'].sum()\n",
    "whale_revenue = df[df['is_whale'] == 1][target].sum()\n",
    "total_revenue = df[target].sum()\n",
    "\n",
    "print(f\"Whale users: {whale_count:,} ({whale_count/len(df)*100:.2f}%)\")\n",
    "print(f\"Whale revenue: ${whale_revenue:,.2f} ({whale_revenue/total_revenue*100:.2f}% of total)\")\n",
    "print(f\"Average whale revenue: ${df[df['is_whale']==1][target].mean():.2f}\")\n",
    "print(f\"Average non-whale revenue: ${df[df['is_whale']==0][target].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whale characteristics\n",
    "whale_features = [\n",
    "    'avg_daily_sessions', 'avg_act_days', 'avg_duration',\n",
    "    'weeks_since_first_seen', 'weekend_ratio'\n",
    "]\n",
    "\n",
    "whale_comparison = df.groupby('is_whale')[whale_features].mean()\n",
    "whale_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize whale vs. non-whale\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(whale_features):\n",
    "    df.boxplot(column=feature, by='is_whale', ax=axes[i])\n",
    "    axes[i].set_title(f'{feature}')\n",
    "    axes[i].set_xlabel('Is Whale')\n",
    "    plt.sca(axes[i])\n",
    "    plt.xticks([1, 2], ['No', 'Yes'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temporal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime\n",
    "df['datetime_parsed'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d-%H-%M')\n",
    "df['date'] = df['datetime_parsed'].dt.date\n",
    "df['hour'] = df['datetime_parsed'].dt.hour\n",
    "df['weekday'] = df['datetime_parsed'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue by date\n",
    "daily_revenue = df.groupby('date')[target].agg(['mean', 'sum', 'count'])\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "\n",
    "axes[0].plot(daily_revenue.index, daily_revenue['mean'])\n",
    "axes[0].set_title('Average Daily Revenue')\n",
    "axes[0].set_ylabel('Mean Revenue ($)')\n",
    "\n",
    "axes[1].bar(daily_revenue.index, daily_revenue['sum'])\n",
    "axes[1].set_title('Total Daily Revenue')\n",
    "axes[1].set_ylabel('Total Revenue ($)')\n",
    "\n",
    "axes[2].bar(daily_revenue.index, daily_revenue['count'])\n",
    "axes[2].set_title('Daily User Count')\n",
    "axes[2].set_ylabel('Users')\n",
    "axes[2].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue by hour\n",
    "hourly_revenue = df.groupby('hour')[target].mean()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(hourly_revenue.index, hourly_revenue.values)\n",
    "plt.title('Average Revenue by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Mean Revenue ($)')\n",
    "plt.xticks(range(24))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue by weekday\n",
    "weekday_revenue = df.groupby('weekday')[target].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(weekday_revenue.index, weekday_revenue.values)\n",
    "plt.title('Average Revenue by Weekday')\n",
    "plt.xlabel('Weekday (0=Monday)')\n",
    "plt.ylabel('Mean Revenue ($)')\n",
    "plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Categorical Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top categories by revenue\n",
    "categorical_cols = ['country', 'dev_os', 'advertiser_category']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        top_10 = df.groupby(col)[target].mean().sort_values(ascending=False).head(10)\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        top_10.plot(kind='barh')\n",
    "        plt.title(f'Top 10 {col} by Average Revenue')\n",
    "        plt.xlabel('Mean Revenue ($)')\n",
    "        plt.ylabel(col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example interaction feature\n",
    "if 'avg_daily_sessions' in df.columns and 'avg_act_days' in df.columns:\n",
    "    df['engagement_score'] = df['avg_daily_sessions'] * df['avg_act_days']\n",
    "    \n",
    "    print(f\"Correlation of engagement_score with target: {df['engagement_score'].corr(df[target]):.4f}\")\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(\n",
    "        df['engagement_score'],\n",
    "        df[target],\n",
    "        alpha=0.3,\n",
    "        s=10\n",
    "    )\n",
    "    plt.xlabel('Engagement Score')\n",
    "    plt.ylabel('Revenue ($)')\n",
    "    plt.title('Engagement vs. Revenue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value percentage\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "missing_pct = missing_pct[missing_pct > 0]\n",
    "\n",
    "if len(missing_pct) > 0:\n",
    "    print(\"Features with Missing Values:\")\n",
    "    print(missing_pct)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_pct.head(20).plot(kind='barh')\n",
    "    plt.xlabel('Missing %')\n",
    "    plt.title('Top 20 Features with Missing Values')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Findings & Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings:\n",
    "\n",
    "1. **Target Distribution**:\n",
    "   - Highly zero-inflated (~85-90% zeros)\n",
    "   - Right-skewed, log-normal distribution\n",
    "   - Top 5% of users account for 50%+ of revenue\n",
    "\n",
    "2. **Buyer Patterns**:\n",
    "   - Low conversion rate (~10-15%)\n",
    "   - Clear separation between buyers and non-buyers\n",
    "   - Two-stage modeling approach recommended\n",
    "\n",
    "3. **Whale Users**:\n",
    "   - Top 5% drive majority of revenue\n",
    "   - Higher engagement, longer retention\n",
    "   - Different behavioral patterns\n",
    "\n",
    "4. **Temporal Patterns**:\n",
    "   - Strong hourly variation (peak: evening)\n",
    "   - Weekend vs. weekday differences\n",
    "   - Stable daily trends\n",
    "\n",
    "5. **Feature Importance**:\n",
    "   - Purchase history features are most predictive\n",
    "   - Behavioral engagement metrics crucial\n",
    "   - Device/geo features moderately important\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Modeling Strategy**:\n",
    "   - Stage 1: Buyer classification (binary)\n",
    "   - Stage 2: Revenue regression (for buyers)\n",
    "   - Stage 3: Ensemble/calibration\n",
    "\n",
    "2. **Sampling**:\n",
    "   - Apply HistOS/HistUS for imbalanced data\n",
    "   - Oversample rare buyers\n",
    "   - Undersample zero-heavy distributions\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Create interaction features (whale Ã— frequency)\n",
    "   - Local distribution features (LDAO paper)\n",
    "   - Recency weighting (exponential decay)\n",
    "   - Temporal cyclical encoding\n",
    "\n",
    "4. **Loss Function**:\n",
    "   - Use Huber loss (robust to outliers)\n",
    "   - Consider MSLE for log-scale optimization\n",
    "   - Multi-task learning for D1, D7, D14\n",
    "\n",
    "5. **Validation**:\n",
    "   - Temporal split (Oct 1-7 train, Oct 7 val)\n",
    "   - Stratified by buyer/non-buyer\n",
    "   - Monitor both AUC (buyer) and MSLE (revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next Steps**:\n",
    "1. Run `scripts/train.py` to train models\n",
    "2. Experiment with feature engineering\n",
    "3. Hyperparameter tuning\n",
    "4. Ensemble strategies\n",
    "5. Generate submission with `scripts/predict.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}