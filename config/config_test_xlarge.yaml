# Test Configuration - XLARGE (Est: ~4-5 minutes, LIMIT!)
# Maximum data while staying under 5 min

data:
  train_path: "data/raw/train/train"
  test_path: "data/raw/test/test"
  processed_path: "data/processed"
  submission_path: "data/submissions"
  
  # XLARGE: 48 hours train (2 days), 12 hours val
  train_start: "2025-10-01-00-00"
  train_end: "2025-10-02-23-00"
  val_start: "2025-10-03-00-00"
  val_end: "2025-10-03-11-00"
  model_start: "2025-10-01-00-00"
  model_end: "2025-10-03-11-00"
  test_start: "2025-10-08-00-00"
  test_end: "2025-10-08-01-00"

features:
  user_behavioral:
    - avg_act_days
    - avg_daily_sessions
    - avg_duration
    - weeks_since_first_seen
    - weekend_ratio
    - wifi_ratio
  
  purchase_history:
    - iap_revenue_usd_bundle
    - iap_revenue_usd_category
    - num_buys_bundle
    - num_buys_category
  
  app_context:
    - advertiser_bundle
    - advertiser_category
    - advertiser_subcategory
    - advertiser_bottom_taxonomy_level
  
  device:
    - dev_make
    - dev_model
    - dev_os
    - dev_osv
    - release_date
    - release_msrp
  
  network:
    - carrier
    - country
    - region
  
  temporal:
    - hour
    - weekday

sampling:
  histos:
    n_bins: 25
    window_size: 10000
    target_percentile: 75
    
models:
  stage1_buyer:
    type: "lightgbm"
    params:
      objective: "binary"
      metric: "auc"
      num_leaves: 31
      learning_rate: 0.05
      feature_fraction: 0.8
      bagging_fraction: 0.8
      bagging_freq: 5
      max_depth: 8
      min_child_samples: 20
      n_estimators: 200
      early_stopping_rounds: 25
      verbose: -1
  
  stage2_revenue:
    type: "odmn"
    horizons: [1, 7, 14]
    params:
      objective: "regression"  # Same as SMALL (working config)
      metric: "rmse"
      num_leaves: 31  # Same as SMALL
      learning_rate: 0.05  # Same as SMALL
      feature_fraction: 0.8  # Same as SMALL
      bagging_fraction: 0.8  # Same as SMALL
      bagging_freq: 5
      max_depth: 7  # Same as SMALL
      min_child_samples: 15  # Same as SMALL
      reg_alpha: 0.05
      reg_lambda: 0.05
      n_estimators: 300  # Scale up trees for maximum data
      early_stopping_rounds: 40
      verbose: -1
    loss:
      lambda_order: 0.1
      lambda_d1: 0.3
      lambda_d7: 0.5
      lambda_d14: 0.2
  
  ensemble:
    type: "stacking"
    base_models:
      - type: "elastic_net"
        alpha: 0.1
        l1_ratio: 0.5
      - type: "random_forest"
        n_estimators: 75
        max_depth: 10
      - type: "xgboost"
        objective: "reg:squarederror"
        max_depth: 7
        n_estimators: 75
    meta_learner:
      type: "lightgbm"
      objective: "huber"
      num_leaves: 31

training:
  batch_size: 10000
  n_folds: 3
  random_state: 42
  use_gpu: false
  cache_features: false
  sampling:
    train_frac: 0.25       # 25% of available data
    val_frac: 0.5          # 50% of available data
    max_train_partitions: 15
    max_val_partitions: 5
    random_state: 42
    fallback_val_rows: 1000

  split:
    strategy: "stratified_random"
    model_start: "2025-10-01-00-00"
    model_end: "2025-10-03-11-00"
    train_fraction: 0.8
    stratify_column: "buyer_d7"
    shuffle: true
    random_state: 42

inference:
  batch_size: 10000
  use_cached_embeddings: false
