# Smadex LTV Prediction Configuration

data:
  train_path: "data/raw/train/train"
  test_path: "data/raw/test/test"
  processed_path: "data/processed"
  submission_path: "data/submissions"
  
  # Temporal splits (trimmed to keep total footprint â‰ˆ5 GB)
  train_start: "2025-10-01-00-00"
  train_end: "2025-10-02-11-00"
  val_start: "2025-10-02-12-00"
  val_end: "2025-10-02-17-00"
  test_start: "2025-10-08-00-00"
  test_end: "2025-10-11-23-00"
  
  # Combined window for stratified split
  model_start: "2025-10-01-00-00"
  model_end: "2025-10-02-17-00"

features:
  # Core feature groups
  user_behavioral:
    - avg_act_days
    - avg_daily_sessions
    - avg_duration
    - weeks_since_first_seen
    - weekend_ratio
    - wifi_ratio
  
  purchase_history:
    - iap_revenue_usd_bundle
    - iap_revenue_usd_category
    - num_buys_bundle
    - num_buys_category
  
  app_context:
    - advertiser_bundle
    - advertiser_category
    - advertiser_subcategory
    - advertiser_bottom_taxonomy_level
  
  device:
    - dev_make
    - dev_model
    - dev_os
    - dev_osv
    - release_date
    - release_msrp
  
  network:
    - carrier
    - country
    - region
  
  temporal:
    - hour
    - weekday

# Revenue preprocessing
preprocessing:
  revenue_cap: 500.0  # Cap extreme outliers at $500 (p99 was $254)
  use_log_target: false  # Use log1p transform for revenue targets (optional)

sampling:
  histos:
    n_bins: 30
    window_size: 50000
    target_percentile: 75
  
  histus:
    n_bins: 30
    target_percentile: 2  # Keep negatives dominant to emphasize zero baseline
    remove_zero_revenue: true  # Remove buyers with $0 revenue
    
models:
  stage1_buyer:
    type: "lightgbm"
    params:
      objective: "binary"
      metric: "auc"
      num_leaves: 31
      learning_rate: 0.05
      feature_fraction: 0.8
      bagging_fraction: 0.8
      bagging_freq: 5
      max_depth: -1
      min_child_samples: 60
      n_estimators: 500
      early_stopping_rounds: 50
      verbose: -1
      lambda_l1: 0.3
      lambda_l2: 0.3
      scale_pos_weight: 8.0  # Stronger penalty for false positives
    calibration:
      method: "beta"
      regularization: 0.05
      epsilon: 1e-4
      max_iter: 500

  high_value_buyer:
    type: "lightgbm"
    target_quantile: 0.995
    min_revenue: 75.0
    params:
      objective: "binary"
      metric: "auc"
      num_leaves: 15
      learning_rate: 0.05
      feature_fraction: 0.6
      bagging_fraction: 0.7
      bagging_freq: 5
      max_depth: 8
      min_child_samples: 40
      n_estimators: 300
      early_stopping_rounds: 40
      verbose: -1
  
  stage2_revenue:
    type: "odmn"  # Order-preserving multi-task
    horizons: [1, 7, 14]
    params:
      objective: "tweedie"
      tweedie_variance_power: 1.1
      learning_rate: 0.015
      num_leaves: 63
      feature_fraction: 0.7
      bagging_fraction: 0.7
      bagging_freq: 5
      n_estimators: 1000
      early_stopping_rounds: 100
      verbose: -1
      min_data_in_leaf: 20  # Require more samples per leaf to stabilize regression
      lambda_l1: 0.5  # Stronger regularization keeps predictions conservative
      lambda_l2: 0.5
      max_delta_step: 5.0
    loss:
      lambda_order: 0.1
      lambda_d1: 0.3
      lambda_d7: 0.5
      lambda_d14: 0.2
    gating:
      enabled: true
      alpha: 2.5
      floor: 0.0
      probability_cap: 0.15
      probability_cutoff: 0.05
      target_zero_rate: 0.975
      min_probability_cutoff: 0.02
      max_probability_cutoff: 0.15
      zero_rate_tolerance: 0.002
    whale_filter:
      enabled: true
      max_revenue: 90.0   # Drop buyers whose D7 revenue exceeds this cap
      percentile: 99.0    # Additionally drop the top 1% even if below absolute cap
    whale_gate:
      enabled: true
      alpha: 1.5
      floor: 0.0
      probability_cap: 0.4
      probability_cutoff: 0.25
  
  ensemble:
    type: "stacking"
    feature_cap: 15.0
    base_models:
      - type: "elastic_net"
        alpha: 0.6
        l1_ratio: 0.7
        max_iter: 5000
      - type: "random_forest"
        n_estimators: 100
        max_depth: 10
      - type: "xgboost"
        objective: "reg:squarederror"
        max_depth: 6
    meta_learner:
      type: "lightgbm"
      objective: "quantile"
      alpha: 0.65
      num_leaves: 31

training:
  batch_size: 10000
  chunk_size: 50000  # Rows per chunk for memory-efficient processing
  use_chunked_loading: true  # Use chunked loading to avoid RAM collapse
  n_folds: 5
  random_state: 42
  use_gpu: false
  cache_features: false
  zero_anchor:
    enabled: true
    rows_per_day: 2
    random_state: 42
  
  # Use stratified random split to avoid distribution shift
  split:
    strategy: "stratified_random"  # Changed from temporal
    train_fraction: 0.8
    stratify_column: "buyer_d7"
    shuffle: true
    random_state: 42
    model_start: "2025-10-01-00-00"
    model_end: "2025-10-02-17-00"
  
  sampling:
    frac: 0.35  # Sample 35% of available train data for manageable runtime
    max_train_partitions: 3  # Cap chunk count to avoid memory spikes
    random_state: 42

inference:
  batch_size: 5000
  use_cached_embeddings: true
  max_partitions: null       # process every available partition for full coverage
  sample_frac: null          # avoid down-sampling so we keep all rows
  limit_rows: null           # no hard cap; required for 14M+ submission