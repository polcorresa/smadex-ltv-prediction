data:
  train_path: data/raw/train/train
  test_path: data/raw/test/test
  processed_path: data/processed
  submission_path: data/submissions
  train_start: 2025-10-01-00-00
  train_end: 2025-10-01-02-00
  val_start: 2025-10-01-03-00
  val_end: 2025-10-01-03-00
  test_start: 2025-10-08-00-00
  test_end: 2025-10-08-01-00
features:
  user_behavioral:
  - avg_act_days
  - avg_daily_sessions
  - avg_duration
  - weeks_since_first_seen
  - weekend_ratio
  - wifi_ratio
  purchase_history:
  - iap_revenue_usd_bundle
  - iap_revenue_usd_category
  - num_buys_bundle
  - num_buys_category
  app_context:
  - advertiser_bundle
  - advertiser_category
  - advertiser_subcategory
  - advertiser_bottom_taxonomy_level
  device:
  - dev_make
  - dev_model
  - dev_os
  - dev_osv
  - release_date
  - release_msrp
  network:
  - carrier
  - country
  - region
  temporal:
  - hour
  - weekday
sampling:
  histos:
    n_bins: 10
    window_size: 1000
    target_percentile: 75
models:
  stage1_buyer:
    type: lightgbm
    params:
      objective: binary
      metric: auc
      num_leaves: 15
      learning_rate: 0.1
      feature_fraction: 0.8
      bagging_fraction: 0.8
      bagging_freq: 5
      max_depth: 5
      min_child_samples: 10
      n_estimators: 50
      early_stopping_rounds: 10
      verbose: -1
  stage2_revenue:
    type: odmn
    horizons:
    - 1
    - 7
    - 14
    params:
      objective: huber
      alpha: 0.9
      num_leaves: 15
      learning_rate: 0.1
      feature_fraction: 0.7
      bagging_fraction: 0.7
      bagging_freq: 5
      n_estimators: 50
      early_stopping_rounds: 10
      verbose: -1
    loss:
      lambda_order: 0.1
      lambda_d1: 0.3
      lambda_d7: 0.5
      lambda_d14: 0.2
  ensemble:
    type: stacking
    base_models:
    - type: elastic_net
      alpha: 0.1
      l1_ratio: 0.5
    - type: random_forest
      n_estimators: 20
      max_depth: 5
    - type: xgboost
      objective: reg:squarederror
      max_depth: 5
      n_estimators: 20
    meta_learner:
      type: lightgbm
      objective: huber
      num_leaves: 15
training:
  batch_size: 1000
  n_folds: 2
  random_state: 42
  use_gpu: false
  cache_features: false
  sampling:
    train_frac: 0.1
    val_frac: 0.2
    max_train_partitions: 2
    max_val_partitions: 1
    random_state: 42
    fallback_val_rows: 100
inference:
  batch_size: 1000
  use_cached_embeddings: false
