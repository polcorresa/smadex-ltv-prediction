# Test Configuration - MEDIUM (Est: ~1-2 minutes)
# More data, more estimators

data:
  train_path: "data/raw/train/train"
  test_path: "data/raw/test/test"
  processed_path: "data/processed"
  submission_path: "data/submissions"
  
  # MEDIUM: 12 hours train, 3 hours val
  train_start: "2025-10-01-00-00"
  train_end: "2025-10-01-11-00"
  val_start: "2025-10-01-12-00"
  val_end: "2025-10-01-14-00"
  model_start: "2025-10-01-00-00"
  model_end: "2025-10-01-14-00"
  test_start: "2025-10-08-00-00"
  test_end: "2025-10-08-01-00"

features:
  user_behavioral:
    - avg_act_days
    - avg_daily_sessions
    - avg_duration
    - weeks_since_first_seen
    - weekend_ratio
    - wifi_ratio
  
  purchase_history:
    - iap_revenue_usd_bundle
    - iap_revenue_usd_category
    - num_buys_bundle
    - num_buys_category
  
  app_context:
    - advertiser_bundle
    - advertiser_category
    - advertiser_subcategory
    - advertiser_bottom_taxonomy_level
  
  device:
    - dev_make
    - dev_model
    - dev_os
    - dev_osv
    - release_date
    - release_msrp
  
  network:
    - carrier
    - country
    - region
  
  temporal:
    - hour
    - weekday

sampling:
  histos:
    n_bins: 15
    window_size: 2000
    target_percentile: 75
    
models:
  stage1_buyer:
    type: "lightgbm"
    params:
      objective: "binary"
      metric: "auc"
      num_leaves: 21
      learning_rate: 0.08
      feature_fraction: 0.8
      bagging_fraction: 0.8
      bagging_freq: 5
      max_depth: 6
      min_child_samples: 15
      n_estimators: 100
      early_stopping_rounds: 15
      verbose: -1
  
  stage2_revenue:
    type: "odmn"
    horizons: [1, 7, 14]
    params:
      objective: "regression"  # Same as SMALL (working config)
      metric: "rmse"
      num_leaves: 31  # Same as SMALL (not 63)
      learning_rate: 0.05  # Same as SMALL (not 0.04)
      feature_fraction: 0.8  # Same as SMALL
      bagging_fraction: 0.8  # Same as SMALL
      bagging_freq: 5
      max_depth: 7  # Same as SMALL
      min_child_samples: 15  # Same as SMALL
      reg_alpha: 0.05
      reg_lambda: 0.05
      n_estimators: 200  # Only increase trees (was 150 in SMALL)
      early_stopping_rounds: 30  # Slightly more patience
      verbose: -1
    loss:
      lambda_order: 0.1
      lambda_d1: 0.3
      lambda_d7: 0.5
      lambda_d14: 0.2
  
  ensemble:
    type: "stacking"
    base_models:
      - type: "elastic_net"
        alpha: 0.1
        l1_ratio: 0.5
      - type: "random_forest"
        n_estimators: 30
        max_depth: 7
      - type: "xgboost"
        objective: "reg:squarederror"
        max_depth: 6
        n_estimators: 30
    meta_learner:
      type: "lightgbm"
      objective: "huber"
      num_leaves: 21

training:
  batch_size: 2000
  n_folds: 3
  random_state: 42
  use_gpu: false
  cache_features: false
  sampling:
    train_frac: 0.15       # 15% of available data
    val_frac: 0.3          # 30% of available data
    max_train_partitions: 5
    max_val_partitions: 2
    random_state: 42
    fallback_val_rows: 200

  split:
    strategy: "stratified_random"
    model_start: "2025-10-01-00-00"
    model_end: "2025-10-01-14-00"
    train_fraction: 0.8
    stratify_column: "buyer_d7"
    shuffle: true
    random_state: 42

inference:
  batch_size: 2000
  use_cached_embeddings: false
